{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from env.custom_hopper import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the neural architecture for value function and agent function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def bootstrapped_discount_rewards(r, gamma, done, next_values):\n",
    "    bootstrapped_discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        if done[t]:\n",
    "            running_add = 0\n",
    "        else:\n",
    "            running_add = r[t] + gamma * next_values[t]\n",
    "        bootstrapped_discounted_r[t] = running_add\n",
    "    return bootstrapped_discounted_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden = 64\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "\n",
    "        self.embedding_ac = torch.nn.Linear(state_space, 512)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc1_ac = torch.nn.Linear(512, 2048)\n",
    "        self.lstm_ac = torch.nn.LSTM(2048, 1024, batch_first=True)\n",
    "       \n",
    "        self.fc2_actor = torch.nn.Linear(1024, action_space)\n",
    "        self.fc2_critic = torch.nn.Linear(1024, 1)\n",
    "\n",
    "        self.sigma_activation = F.softplus\n",
    "        init_sigma = 0.5\n",
    "        self.sigma = torch.nn.Parameter(torch.zeros(self.action_space) + init_sigma)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_ac(x)\n",
    "        x = torch.sum(x, dim=1)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1_ac(x)\n",
    "        x = self.relu(x)\n",
    "        x, _ = self.lstm_ac(x.unsqueeze(0))\n",
    "\n",
    "        action_mean = self.fc2_actor(x.squeeze(0))\n",
    "        action_sigma = self.sigma_activation(self.sigma)\n",
    "        normal_dist = Normal(action_mean, action_sigma)\n",
    "\n",
    "        value = self.fc2_critic(x.squeeze(0))\n",
    "        \n",
    "        return normal_dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our AC is compatible with SB3's PPO, we create a wrapper for our agent by extending ActorCrticPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomActorCriticPolicy, self).__init__(*args, **kwargs)\n",
    "        self.policy = Policy(self.observation_space.shape[0], self.action_space.shape[0])\n",
    "\n",
    "    def _build_mlp_extractor(self):\n",
    "        pass\n",
    "    #Since SB3's ActorCriticPolicy expects an MLP (multi-layer perceptron) feature extractor to be defined  we override the method with a pass statement because we are directly using your custom Policy network, which includes the feature extraction and action/value prediction.\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, obs):\n",
    "        normal_dist, value = self.policy(obs)\n",
    "        return normal_dist.mean, normal_dist.stddev, value\n",
    "\n",
    "    def _predict(self, obs, deterministic=False):\n",
    "        normal_dist, value = self.policy(obs)\n",
    "        if deterministic:\n",
    "            action = normal_dist.mean\n",
    "        else:\n",
    "            action = normal_dist.sample()\n",
    "        action_log_prob = normal_dist.log_prob(action).sum()\n",
    "        return action, value, action_log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include L2 regularization - also know as weight decay- in the PPO setup, you need to add weight decay to the optimizer. Stable Baselines3 doesn't provide a direct way to specify weight decay in the high-level PPO class, but you can customize the policy_kwargs to include it in the optimizer setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr, weight_decay):\n",
    "        super(CustomAdam, self).__init__(params, lr=lr, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(policy, learning_rate):\n",
    "        # Extract actor and critic parameters\n",
    "        actor_params = [param for name, param in policy.named_parameters() if 'actor' in name]\n",
    "        critic_params = [param for name, param in policy.named_parameters() if 'critic' in name]\n",
    "        \n",
    "        # Create optimizers with weight decay for L2 regularization\n",
    "        optimizer_actor = CustomAdam(actor_params, lr=learning_rate, weight_decay=1e-6)\n",
    "        optimizer_critic = CustomAdam(critic_params, lr=learning_rate, weight_decay=1e-6)\n",
    "        \n",
    "        return optimizer_actor, optimizer_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "        features_extractor_class=CustomActorCriticPolicy,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        #env,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        gamma=0.998,\n",
    "        gae_lambda=0.95,\n",
    "        ent_coef=0.01,  # To vary this, you'd need to use a schedule\n",
    "        clip_range=0.2,\n",
    "        n_steps=5120 * 10,\n",
    "        batch_size=5120,\n",
    "        n_epochs=3,\n",
    "        learning_rate=3e-4,  # To vary this, you'd need to use a schedule\n",
    "        vf_coef=1,\n",
    "        max_grad_norm=0.5,\n",
    "        tensorboard_log=\"./ppo_hopper_tensorboard/\",\n",
    "        verbose=1,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
